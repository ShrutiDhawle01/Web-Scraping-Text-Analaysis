# -*- coding: utf-8 -*-
"""Blackcoffer_Data_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yo2-H_x447HA4hys6OxE6-2opAMbOrhl

#Web Scrapping To Text Files
"""

#To extract articles in file
import pandas as pd

df = pd.read_csv('/content/Book1.csv')
x = []
for i in df.url:
  x.append(i)

#Article Extraction

from bs4 import BeautifulSoup
import requests

urls = x 
i=0
for url in urls:
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}
    r=requests.get(url,headers=headers).text
    soup= BeautifulSoup(r, 'lxml')
    file=open('/content/sample_data/scrapped_data/%i.txt' %i, 'w')    
    for article_body in soup.find_all(['h1','p']):
        body=article_body.text
        file.write(body)
    file.close()
    i = i+1


#Converting All Text Files into Zip File
from zipfile import ZipFile
import os
from os.path import basename
# create a ZipFile object
with ZipFile('Extracted_Article.zip', 'w') as zipObj:
   # Iterate over all the files in directory
   for folderName, subfolders, filenames in os.walk("/content/scrapped_data/"):
       for filename in filenames:
           #create complete filepath of file in directory
           filePath = os.path.join(folderName, filename)
           # Add file to zip
           zipObj.write(filePath, basename(filePath))

#Converting csv column into a List of URL's
import pandas as pd

df = pd.read_csv('/content/Book1.csv')
x = []
for i in df.url:
  x.append(i)

"""#Extracting Article With Title Using BeautifulSoup"""

#Extracting  Title
from bs4 import BeautifulSoup
import requests

urls = x 
data=[]
for url in urls:
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}
    r=requests.get(url,headers=headers).text
    soup= BeautifulSoup(r, 'lxml')
    for tag in soup.find_all('h1', class_='entry-title'):
      body =tag.get_text()
      data.append(body)

from bs4 import BeautifulSoup
import requests

urls = x 
i=0
dat=[]
for url in urls:
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}
    r=requests.get(url,headers=headers).text
    soup= BeautifulSoup(r, 'lxml')
    for tag in soup.find_all(class_=['td-post-content']):
      body =tag.get_text()
      dat.append(body)

#Combining title and paragraph
A = data
B = dat

final_dat = [' '.join(z) for z in zip(A, B)]
print(final_dat[2])

df = pd.read_csv("Book12.csv")
df["text"] = final_dat
df.to_csv("Book12.csv", index=False)

df.head(171)

#Reading csv file containing extracted text from url's
import pandas as pd
data=pd.read_csv('Book12.csv')
data.head()

"""#Removing Stopwords, Punctuations"""

import nltk
nltk.download('stopwords')

import nltk
nltk.download('punkt')

#Converting extra stopwords from a text file to list
my_file = open("/content/StopWords_Generic.txt", "r")
content_list = my_file.readlines()
print(content_list)

#Removing Neline Character(\n)
a=[]

for i in content_list:
  a.append(i.replace('\n',' '))

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

def remove_punction_and_stopwords(msg):
   stop_words = set(stopwords.words('english')+a)
   word_tokens = word_tokenize(msg)
   filtered_words = [w for w in msg.lower().split() if w not in stop_words]
   new_sentence = ' '.join(filtered_words)
   return new_sentence

data_clean = data['text'].apply(remove_punction_and_stopwords)

data_clean[0]

"""#Removing Punctuations"""

import string

new_dat_clean = [''.join(c for c in s if c not in string.punctuation) for s in data_clean]

new_dat_clean[0]

df = pd.read_csv("Book12.csv")
df["cleaned_text"] = new_dat_clean 
df.to_csv("Book12.csv", index=False)

#Positve Negative words list
#Converting text file to list
import pandas as pd

column_namesn = ["Word", "Negative"]
column_namesp = ["Word","Positive"]
dat1 = pd.read_csv("/content/pn.csv", usecols=column_namesn)
data1 = pd.read_csv("/content/pn.csv", usecols=column_namesp)
dat1

#Dropping rows that are non-negative or positve separately
dat1.drop(dat1.index[(dat1["Negative"] == 0)],axis=0,inplace=True)
data1.drop(data1.index[(data1["Positive"] == 0)],axis=0,inplace=True)

#Conversion to list
n = dat1.Word.to_list()
p = data1.Word.to_list()
negat =[x.lower() for x in n]
posit = [x.lower() for x in p]
print("List of Negative Words:",negat)
print("List of Positive Words:",posit)

"""#Adding and finding positve and negative score to output csv file"""

output_strc = pd.read_csv("/content/Output Data Structure.xlsx - Sheet1.csv")

num_pos = df['cleaned_text'].map(lambda x: len([i for i in x.split() if i in posit]))
output_strc['POSITIVE SCORE'] = num_pos
num_neg = df['cleaned_text'].map(lambda x: len([i for i in x.split() if i in negat]))
output_strc['NEGATIVE SCORE'] = num_neg

df.head()

output_strc.head()

"""#Finding Polarity Score"""

def x(Positive_Score,Negative_Score):
    return (Positive_Score-Negative_Score)/ ((Positive_Score + Negative_Score) + 0.000001)

output_strc['POLARITY SCORE']  = output_strc.apply(lambda f: x(f['POSITIVE SCORE'],f['NEGATIVE SCORE']), axis=1)

output_strc.head()

"""#Finding Subjectivity Score"""

words_aftr_clean = new_dat_clean
words_aftr_clean[0:5]

#Counting total words
ct= []
for item in words_aftr_clean:
    ct.append(len(item.split()))

#Adding totals words after cleaning to a csv file
df["tc"] = ct

df['pos_count'] = num_pos
df['neg_count'] = num_neg

df.head()

def z(Positive_Score,Negative_Score,tot_count):
    return (Positive_Score + Negative_Score)/ ((tot_count) + 0.000001)

df['Subjectivity Score']  = df.apply(lambda f: z(f['pos_count'],f['neg_count'],f['tc']), axis=1)

output_strc['SUBJECTIVITY SCORE']  = df['Subjectivity Score']

df['POLARITY SCORE']  = df.apply(lambda f: x(f['pos_count'],f['neg_count']), axis=1)

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

def sentences_count(ms):
  number_of_sentences = sent_tokenize(ms)
  return len(number_of_sentences)

sent_count = df['text'].apply(sentences_count)

df['Total Sentences in text'] = sent_count

"""#Calculating total words in original text"""

ze =[]
for i in range(len(final_dat)):
  ze.append(len(final_dat[i].split()))

df['Total No Words in text'] = ze

"""#Calculating total characters in text"""

wrd_lst = []
for i in final_dat:
  wrd_lst.append(len(i))

df['Total characters in text'] = wrd_lst

"""#Average Sentence Length

"""

def q(no_of_wrds,no_of_sent):
    return no_of_wrds / no_of_sent

df['Avg_Sentence_Len']  = df.apply(lambda f: q(f['Total No Words in text'],f['Total Sentences in text']), axis=1)

output_strc['AVG SENTENCE LENGTH']  = df['Avg_Sentence_Len']

"""#Average Number of Words Per Sentence"""

def q(no_of_wrds,no_of_sent):
    return no_of_wrds / no_of_sent

df['Avg_no_wrd_per_Sent']  = df.apply(lambda f: q(f['Total No Words in text'],f['Total Sentences in text']), axis=1)

output_strc['AVG NUMBER OF WORDS PER SENTENCE']  = df['Avg_no_wrd_per_Sent']

"""#Word Count"""

output_strc['WORD COUNT']  = df['tc']

"""#Average Word Length"""

def q(no_of_char,no_of_wrd):
    return no_of_char / no_of_wrd

df['Avg_Word_Length']  = df.apply(lambda f: q(f['Total characters in text'],f['Total No Words in text']), axis=1)

output_strc['AVG WORD LENGTH']  = df['Avg_Word_Length']

"""#Count personal pronouns"""

def count_pronouns(test_str):
  regex = r"(\b(s?he|it|i|ours|we|us|my)\b)"
  matches = re.finditer(regex, test_str, re.MULTILINE)
  track = 0
  for matchNum, match in enumerate(matches, start=1):
      track+=1
  return track

pro_count = df['text'].apply(count_pronouns)

df['pronoun_count'] = pro_count

output_strc['PERSONAL PRONOUNS']  = df['pronoun_count']

#Total Syallble count 

def count(s, c) :
    res = 0
     
    for i in range(len(s)) :
        if (s[i] == c):
            res = res + 1
    return res

lstm = []


vowels = ['a','e','i','o','u']
str2,str3 = 'es','ed'
for i in final_dat:
  cv = 0
  sdf = i.split()
  for j in sdf:
    flag = 0
    for k in vowels:
      z = count(j,k)
      cv = cv + z
      
      if flag ==0:
        if str2 in j[len(j)-2:] or str3 in j[len(j)-2:]:
           cv = cv - 1
           flag = 1
  lstm.append(cv)

df['Tot_syallble_count'] = lstm

output_strc['SYLLABLE PER WORD'] =  df['Tot_syallble_count']

"""#COMPLEX WORD COUNT"""

def count(s, c) :
    res = 0
     
    for i in range(len(s)) :
        if (s[i] == c):
            res = res + 1
    return res

complex_count = []
vowels = ['a','e','i','o','u']
str2,str3 = 'es','ed'
for i in final_dat:
  d = 0
  cv = 0
  sdf = i.split()
  for j in sdf:
    cv = 0
    for k in vowels:
      z = count(j,k)
      cv = cv + z
      
    if cv>2:
      d = d+1
  complex_count.append(d)

output_strc['COMPLEX WORD COUNT'] = complex_count

df['COMPLEX_WORD_COUNT'] = complex_count

"""#Percentage of Complex words"""

def percent_complex(no_com_wrd, no_wrd):
  return no_com_wrd / no_wrd

output_strc['PERCENTAGE OF COMPLEX WORDS']  = df.apply(lambda f: percent_complex(f['COMPLEX_WORD_COUNT'],f['Total No Words in text']), axis=1)

Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)

"""# Fog Index

"""

def Fog_index(asl, pcw):
  return 0.4 * (asl + pcw)

output_strc['FOG INDEX']  = output_strc.apply(lambda f: Fog_index(f['AVG SENTENCE LENGTH'],f['PERCENTAGE OF COMPLEX WORDS']), axis=1)

"""#Final Output File"""

output_strc.head()

output_strc.to_excel('OUTPUT.xlsx')